---
layout: post
title: "Data & AI Trends in 2025"
date: 2025-02-21
author: "Jamin Chen"
header-img: "img/post-bg-universe.jpg"
tags: ["Artificial Intelligence"]
---

## Background

The history of human society, like the history of science and technology, advances in a spiral. Since the concept of Artificial Intelligence was proposed in the 1950s, AI technology has gone through multiple cycles of spring and winter ([AI Winter](https://en.wikipedia.org/wiki/AI_winter)). The previous generation of AI technology (1970s-1980s) was systems, Japan's fifth-generation computer system, etc. These based on symbolic ( technologies had advantages insymbolic) computing knowledge representation and logical systems, represented by LispMachine, expert reasoning but lacked pattern recognition capabilities. Subsequently, the revival of statistics-based Machine Learning (1990s-2000s) occurred; although AI was not yet widespread at that time, related data statistical analysis, big data systems, and cloud computing infrastructure developed rapidly during this period. After Hinton et al. proposed the "deep learning" concept in 2006, with the success of AlexNet (2012) and AlphaGo (2015), AI technology gradually evolved from shallow statistical models (e.g., SVM) to deep neural networks (e.g., CNN). In 2017, researchers at Google proposed the [Transformer architecture](https://arxiv.org/abs/1706.03762), after which the complexity of neural networks increased exponentially, driving technological changes in NLP and related fields. OpenAI achieved a milestone innovation in AI in 2022 based on the Transformer architecture (ChatGPT), reigniting humanity's enthusiasm and desire for AGI. This article analyzes the cutting-edge Data&AI technologies and trends for 2025 based on the AI technology highlights and development threads of the past two to three years, and discusses the author's thoughts on the path to achieving AGI.

## A Brief Intro to Transformer

The Transformer is one of the architectures of deep neural networks, specifically belonging to the **unsupervised self-regressive pretraining** model. Unsupervised (Self-supervised) means no manual labeling of data is required; the model automatically learns intermediate feature representations and possesses transfer learning capabilities. **Self-regressive** (Auto-regressive) is one of the methods for pretraining generative language models, whose essence lies in "regression," predicting the future based on past data, such as LLM predicting the next token. **Pretraining** refers to fitting the model with a large number of random samples, which does not yet possess (efficient) problem-solving capabilities for specific scenarios, so it requires a post-training stage to fine-tune the model to adapt to downstream problems and scenarios.

Related to AR is another pretraining method called **auto-encoding**, such as the masked language model proposed by [BERT](https://arxiv.org/abs/1810.04805), where information at random positions in the sample is masked, allowing the model to learn rich linguistic symbol representations by combining contextual information, facilitating downstream reuse.

AR and AE are two commonly used pretraining methods for generative language models, but they are not all-encompassing. For example, [XLNet](https://arxiv.org/abs/1906.08237) proposes permutation language models to combine bidirectional contexts, possessing the advantages of both AR and AE. Additionally, although **reinforcement learning** and **adversarial learning** have played important roles in the training of some LLMs (such as DeepSeek-R1), their core training algorithms and application scenarios still require further exploration. In the graph computation domain, while AR and AE are also used to train low-dimensional representations of graph nodes and achieve good results in downstream graph tasks, native graph training methods are still under investigation.

AR and AE models have achieved great success in the NLP field. Looking beyond appearances, in current language model training, learning is not based on the semantics (concepts) of natural language units but rather on the statistical patterns of token combinations. In other words, in the sequence space expanded by the token set, actual natural language samples are a subset of this space. The Transformer captures the sequence patterns of this subset through neural network parameters. As long as the parameter space represented by the neural network sufficiently covers the sample sequence space, it can successfully learn the representation of the subset. In this sense, the training methods of AR and AE have universal learning capabilities for "sequence spaces of finite element sets."

The Transformer architecture has been practically applied and reached SOTA in multiple domains beyond natural language:

* Computer Vision: Image classification [[ViT](https://arxiv.org/abs/2010.11929)], Object detection [[DETR](https://arxiv.org/abs/2005.12872)], Image generation [[TransGAN](https://arxiv.org/abs/2102.07074)], Image segmentation [[SETR](https://arxiv.org/abs/2012.15840)]
* Bioinformatics: Protein structure prediction [[AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2)], Gene sequences [[Lee2022](https://www.nature.com/articles/s41467-022-34152-5)], Single-cell genomics [[Szalata2024](https://www.nature.com/articles/s41592-024-02353-z)]
* Time Series Analysis: Weather prediction [[FuXi](https://www.nature.com/articles/s41612-023-00512-1)], Spatiotemporal dynamics prediction [[Li2024](https://www.nature.com/articles/s41467-022-34152-5)], Financial forecasting [[Galformer](https://www.nature.com/articles/s41598-024-72045-3)]
* Speech Processing: Speech recognition [[Conformer](https://arxiv.org/abs/2005.08100)], Synthesis [[Tacotron](https://arxiv.org/abs/1712.05884)]
* Multimodal Learning: Text-image generation [[DALL-E](https://arxiv.org/abs/2102.12092)], Video understanding [[VideoBERT](https://arxiv.org/abs/1904.01766)]

From the above practices, it can be seen that by appropriately converting domain-specific problems into sequence data and constructing discrete element sets, [Mirchandani2023](https://arxiv.org/pdf/2307.04721) summarizes that the Transformer is a **universal sequence pattern learner**. For a closed or semi-closed complex system, when the system has a finite state set or finite state generation rules (such as explicit physical laws), the Transformer can effectively model the behavior of such systems.

## Technology and Innovation Trends

### LLM Engineering Optimization

The logic behind the extreme optimization of Transformers is everyone's belief in scaling laws. With the increase in training data scale and parameter quantity (trillion-scale), people expect to achieve another emergent moment similar to GPT-3 through extreme engineering optimization within limited resources.

In this context, on the one hand, there is the brute-force stacking of compute cards, and on the other hand, teams like DeepSeek optimize every part of the Transformer: computational efficiency (e.g., attention mechanism, long-sequence optimization, parallel training), memory usage (e.g., floating-point precision, attention window), inference/Inference optimization (e.g., KVCache, distillation, quantization, wafer-level acceleration), etc.

Engineering optimization is a necessary path for the productization and business application of LLMs to reduce costs, but simply pursuing scaling laws cannot bring us closer to AGI or human-level intelligence, which is an inherent limitation of the Transformer and neural network algorithms (see the post-LLM era below).

> So, no matter how loudly Jason Huang emphasizes the importance of scaling laws, researchers must maintain rationality and insight, understand, master, and transcend scaling laws to find the key that truly opens the door to AGI.

### LLM Knowledge Enhancement

There are several significant flaws in the LLM architecture:

* **High cost of knowledge updates**: This is a limitation of the latent knowledge representation (LKR) based on neural network parameters, making it impossible to reuse, locally update, or transfer knowledge. Understanding the LKR of LLMs is an important issue in explainable AI research. Finding general knowledge representations or solving this challenge is also one of the technical difficulties that AGI needs to overcome.
* **Generation of "hallucinations"**: Although the attention modules of LLMs are deterministic—i.e., after the model is trained, the output for any given input to an attention module is fixed—the hallucination problem still exists due to various reasons [[Huang2025](https://dl.acm.org/doi/abs/10.1145/3703155)], such as defects in training data, error propagation, and weak consistency due to lack of self-reference.
* **Insufficient explainability and controllability**: This is also a fundamental limitation of neural network architectures. Almost all post-training work can only be done end-to-end. Although related LLM probing and [editing](https://arxiv.org/abs/2202.05262) techniques are being studied, there is still no stable and reliable methodology or tools. Overfitting and generalization in neural network models, like Venus and Hesperus in the night sky, are inherently contradictory yet interdependent.

Despite these issues, LLMs, as a disruptive technology in natural language processing, have immense potential to enhance social productivity. To utilize updated information or private domain data, the workflow of Retrieval-Augmented Generation (RAG) has been proposed. From the initial vector-based [VectorRAG](https://arxiv.org/abs/2005.11401) to the more semantically rich [GraphRAG](https://arxiv.org/abs/2404.16130), 2024 was the year of RAG, and related technologies have become the foundation for commercializing LLMs and essential skills for future AI-native applications.

At the same time, combining data and algorithm enhancements reduces the size of the model while maintaining the effective knowledge density of LLMs (the author defines effective knowledge density as the ratio of the completeness of knowledge in LLMs to the model complexity—equivalent to the parameter count of vanilla Transformer architecture), paving the way for enhancing reasoning quality and depth. As Ilya Sutskever stated, the Internet is the fossil fuel of the LLM era. In cases of limited data scale, thoroughly understanding LLM principles and refining algorithms is the right path (see the section on explainability below).

### From Instant Inference to Deep Reasoning

Since the proposal of [Chain of Thought](https://arxiv.org/abs/2201.11903), people have been optimizing LLMs' ability to solve complex reasoning problems while continuously exploring how to simulate human thinking through computers (Turing's imitation game). The breakout of DeepSeek, on the one hand, reflects the pursuit of engineering excellence, but more importantly, it open-sourced the training methods for GPT-4-level models with deep reasoning capabilities. People enjoy free GPT-4-level models while researchers no longer need to guess the internal training mechanisms.

If inference is a shallow feedback of the implicit knowledge representation (token sequence patterns) captured by the model during the training phase, then reasoning is a deeper mining of these implicit knowledge representations. Natural language-based reasoning generates new knowledge, with recent achievements including OpenAI o1/o3-mini, DeepSeek-R1, and Google Gemini Flash Thinking. Dr. Wolfe recently wrote an excellent [summary article](https://web.archive.org/web/20250219080523/https://cameronrwolfe.substack.com/p/demystifying-reasoning-models).

Deep research into reasoning mechanisms and algorithms might be one of the most valuable assets left by the AI renaissance led by ChatGPT for future AGI science, and also the core competitiveness of LLM commercialization. Deep reasoning for LLMs is akin to the PageRank algorithm for search engines. Commercial giants use LLM technology to integrate ubiquitous internal data silos and develop more attractive product experiences based on deep reasoning. The recently launched Gemini Flash Thinking with apps functionality, integrating YouTube and Google Maps data, demonstrates stronger practical problem-solving capabilities than pure model reasoning or RAG. However, because it involves core business logic, the underlying deep reasoning chains may no longer adopt an open-source approach in the future.

### LLM Explainability

Compared to simply using LLMs well, understanding the working mechanisms of LLMs and deeply exploring foundational technologies related to knowledge representation, memory, reasoning, forgetting, etc., linked to human cognition, is more conducive to advancing towards AGI. Therefore, AI pioneers loudly call for attention to neuroscience and AI cross-domain research (NeuroAI) in [Nature](https://www.nature.com/articles/s41467-023-37180-x), strengthening education and funding in cognitive and neuroscience foundational fields.

The more layers in a neural network, the larger the parameter space, the more information it can store. In neural networks, patterns are represented as high-dimensional vectors, known as implicit knowledge representations in LLMs. Due to the large volume of training corpus, even with data cleaning and noise reduction, consistent original knowledge cannot be obtained. Practice shows that data bias and artificially constructed attack data can easily influence model behavior.

In terms of understanding LLM mechanisms, [Zhou2024](https://www.nature.com/articles/s41586-024-07930-y) found that LLMs with larger parameters and more instruction tuning during the post-training phase are less stable. Under uncontrollable internal neuron activations, fine-tuning/filtering for specific domain problems accelerates the forgetting of knowledge in other domains. [Mahowald2024](https://www.sciencedirect.com/science/article/pii/S1364661324000275) revealed that LLMs are strong in formal language abilities (i.e., mastering language rules and statistical patterns, such as translation) but weaker in functional language abilities (i.e., applying language in real-world scenarios, such as reasoning), which aligns with the current trend of LLMs evolving towards deep reasoning.

In fact, due to inconsistencies in training data knowledge, [Xu2024](https://aclanthology.org/2024.emnlp-main.486/) showed that such conflicts are pervasive and inherent, requiring more fundamental changes. Therefore, LLMs are weakly consistent models. Related to consistency is Gödel's incompleteness theorem, which states that for any formal system, completeness and consistency cannot coexist. [Pérez2021](https://jmlr.org/papers/v22/20-302.html) proved that Transformer is Turing-complete under certain constraints, and Turing-complete systems are formal systems. Thus, LLMs satisfy Gödel's incompleteness theorem under certain constraints; pursuing a high level of knowledge (high completeness) inevitably disrupts the consistency of knowledge representation: this is akin to a binding spell, defining the capability boundary of a universal single LLM model. Therefore, the author believes that the **MoE architecture** has more potential than the currently popular universal single model.

![neuroai-llm](/img/inpost/2025/neuroai-llm-brain.png)

Moreover, even though LLMs are black boxes, their capture of natural language patterns, especially those involving logic-based reasoning, makes people curious about the essence of this human-like thinking mechanism. In the NeuroAI field, combining LLMs with brain fMRI data research, [Mischler2024](https://www.nature.com/articles/s42256-024-00925-4) and [Kumar2024](https://www.nature.com/articles/s41467-024-49173-5) revealed macroscopic similarities between LLM mechanisms and neural signal processing in the human cortex. Although still in its infancy, research in this field could be a breakthrough for the next generation of AGI technology.

### Post-LLM Era: Two Potential AGI Paths

Learning from others' strengths can help improve our own. In recent years, through in-depth research on LLMs and the Transformer architecture, while enhancing the practicality of LLMs through engineering, people have gradually recognized their intrinsic limitations. Since the conception of AI, people have continuously practiced the idea of "how machines can simulate humans," and classic research areas include cognitive frameworks (Cognitive Architectures), formal systems (Formal Systems), theory of minds (Theory of Minds), world models (World Models), etc. Although these areas' problems have not been fully solved in practice, their ideas still provide inspiration for AGI research.

Based on the current data scale and computing power enhancement, we should revisit the history of AI development, draw on the ideas and practical experiences of predecessors, and explore potential AGI implementation paths from a macro perspective. The author believes that there are two potential AGI implementation paths worth paying attention to: World Models and Neuro-symbolic AI.

![world-model](/img/inpost/2025/world-model-lecun.png)

The concept of **World Models** is consistent with the development of AI. Its core idea starts from first principles, describing the state and behavior of the world and defining the relationship between states and behaviors to construct AI models/systems. World models can be used to solve various problems, such as planning, decision-making, and actions of intelligent agents in environments. [Han2018](https://arxiv.org/abs/1803.10122) first systematically introduced relevant concepts and demonstrated the potential of world models through a vision-memory-controller architecture in a 2D car game. [LeCun2022](https://openreview.net/pdf?id=BZ5a1r-kVsf) proposed the implementation idea of world models based on energy-based models (EBM) and JEPA architecture, which has been put into practice in visual representation and autonomous driving. Feifei Li's team founded [WorldLabsAI](https://www.world
