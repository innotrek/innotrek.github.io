---
layout: post
title: "Data and AI Trends in 2024"
date: 2025-01-01
author: "Jamin Chen"
header-img: "img/post-bg-universe.jpg"
tags: ["Artificial Intelligence"]
---

As we approach the end of 2024, the rapid adoption and widespread dissemination
of large language model (LLM) technology have been remarkable, influencing
companies, communities, and regions worldwide since the initial release of
ChatGPT on November 30, 2022. For someone like me who missed the internet boom
of the late 1990s and early 2000s, this moment feels like a rare opportunity to
witness a transformative technological revolution. The experience has been both
magical and paradoxical—marked by enthusiasm, skepticism, blindness, and
disorder. In this post, I aim to summarize the key developments, trends, and
potential opportunities in data and AI for 2024, drawing insights from both
academic and industrial domains.

## Theories

The focus in AI research continues to center on advancing transformer
architectures—the foundational technology that powers ChatGPT and other (semi-)
open-source LLMs developed by leading tech companies globally. The transformer
framework represents a confluence of innovations: artificial neural networks
conceptualized in the 1950s, the back-propagation algorithm introduced in the
1970s, parallel computing enabled by GPUs, and significant advancements in
software engineering with tools like PyTorch. In this context, the breakthrough
in LLMs is an architecture-driven achievement that leverages the scaling
potential of modern software and hardware.

However, discussions about the limits of scaling laws are gaining traction
within AI communities, reflecting growing unease about the theoretical
frontiers of AI. These concerns highlight the need to address persistent
challenges in LLMs, such as hallucinations, the depletion of high-quality
datasets on the internet, limited self-explanation and reasoning capabilities,
and the escalating energy demands of scaling. The call for theoretical
innovation is becoming increasingly urgent as researchers and practitioners
seek solutions to these pressing issues.

### Understanding the nature of GPT models (Explainable LLM)

**1. Inspecting GPT models regarding to bio-intelligence.**

LLMs provide us a new test bed to inspect how intelligence works. Beyond a long
list of benchmarks and various competing arenas, comparing underlying mechanism
of transformer attentions and human brains cortices offers a novel perspective.

Two recent works [1][2] on Nature report the similar functional specification
of human brain's cortex regions and transformer model layers as well as
attention heads.

[1] Contextual Feature Extraction Hierarchies Converge in Large Language Models
  and the Brain, Nature Machine Intelligence, 2024

[2] Shared functional specialization in transformer-based language models and the
  human brain, Nature Communications, 2024

[3] Generating meaning: active inference and the scope and limits of passive AI,
  Trends in Cognitive Sciences, 2024

**2. Investigating GPT models internal representation.**

Research on sequential data and vector representations:
1995, SVM
2014, word2vec
prior to 2017, RNN/LSTM (invented in 1986 & 1997 respectively)
2017, transformer
2019, BERT, GPT-2
2020, GPT-3

Embedding: static vs. transformed.
Human: baby - limited language input - transformed neuro network
ANN:
NN arch - trained transformation - new input
Maybe future, static kernal net - limited input - transformed artificial net
Neural Language Processing Circuit.

A neurocomputa-tional theory of natural language processing must ultimately
specify how meaning is constructed across words. The Transformer archi-tecture
provides explicit access to a candidate mechanism for quantifying how the
meaning of past words is incorporated into the meaning of the current word.\[2]

Closed-world sequential problems:
weather forcast
protain folding
Crystal formation

None CWSP
Math and physics theories
High-level problem solving strategy (thinking)

**Related papers:**

* Xu, R. et al. (2024) ‘Knowledge Conflicts for LLMs: A Survey’, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Miami, Florida, USA: Association for Computational Linguistics, pp. 8541–8565. Available at: https://doi.org/10.18653/v1/2024.emnlp-main.486.

* Zhou, L. et al. (2024) ‘Larger and more instructable language models become less reliable’, Nature, 634(8032), pp. 61–68. Available at: https://doi.org/10.1038/s41586-024-07930-y.

* Yang, S. et al. (2024) ‘Law of Vision Representation in MLLMs’. arXiv. Available at: https://doi.org/10.48550/arXiv.2408.16357.

* Dohare, S. et al. (2024) ‘Loss of plasticity in deep continual learning’, Nature, 632(8026), pp. 768–774. Available at: https://doi.org/10.1038/s41586-024-07711-7.

* Li, Y. et al. (2024) ‘The Geometry of Concepts: Sparse Autoencoder Feature Structure’. arXiv. Available at: https://doi.org/10.48550/arXiv.2410.19750.

* Orgad, H. et al. (2024) ‘LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations’. arXiv. Available at: https://doi.org/10.48550/arXiv.2410.02707.

### Pursuing potential novel architectures

There are several theory explorations in varying directions during the past
year. \[6,7,13,14,17]

Generative models, sequential inference mode, where inference entails
maximizing accuracy while minimizing complexity.

**1. LSTM revised: xLSTM**

* Beck, M. et al. (2024) ‘xLSTM: Extended Long Short-Term Memory’. arXiv. Available at: https://doi.org/10.48550/arXiv.2405.04517.

**2. Reconciled Polynomial Networks**

* Zhang, J. (2024a) ‘RPN: Reconciled Polynomial Network Towards Unifying PGMs, Kernel SVMs, MLP and KAN’. arXiv. Available at: https://doi.org/10.48550/arXiv.2407.04819.

* Zhang, J. (2024b) ‘RPN 2: On Interdependence Function Learning Towards Unifying and Advancing CNN, RNN, GNN, and Transformer’. arXiv. Available at: https://doi.org/10.48550/arXiv.2411.11162.

**3. Kolmogorov-Arnold Networks**

* Liu, Z., Wang, Y., et al. (2024) ‘KAN: Kolmogorov-Arnold Networks’. arXiv. Available at: https://doi.org/10.48550/arXiv.2404.19756.

* Liu, Z., Ma, P., et al. (2024) ‘KAN 2.0: Kolmogorov-Arnold Networks Meet Science’. arXiv. Available at: https://doi.org/10.48550/arXiv.2408.10205.

## Engineering

### A swarm of RAGs

* Edge, D. et al. (2024) ‘From Local to Global: A Graph RAG Approach to Query-Focused Summarization’. arXiv. Available at: https://doi.org/10.48550/arXiv.2404.16130.

* Chen, S.-A. et al. (2024) ‘TableRAG: Million-Token Table Understanding with Language Models’. arXiv. Available at: https://doi.org/10.48550/arXiv.2410.04739.

* Biswal, A. et al. (2024) ‘Text2SQL is Not Enough: Unifying AI and Databases with TAG’. arXiv. Available at: https://doi.org/10.48550/arXiv.2408.14717.

* Wu, S. et al. (2024) ‘STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases’. arXiv. Available at: https://doi.org/10.48550/arXiv.2404.13207.

### When LLM Meets Graph

* Ren, X. et al. (2024) ‘A Survey of Large Language Models for Graphs’, in Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. KDD ’24: The 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Barcelona Spain: ACM, pp. 6616–6626. Available at: https://doi.org/10.1145/3637528.3671460.

### When LLM Meets Symbolic

* Dong, H. et al. (2024) ‘A Language Model Based Framework for New Concept Placement in Ontologies’, in A. Meroño Peñuela et al. (eds) The Semantic Web. Cham: Springer Nature Switzerland (Lecture Notes in Computer Science), pp. 79–99. Available at: https://doi.org/10.1007/978-3-031-60626-7\_5.

* Giglou, H.B. et al. (2024) ‘LLMs4OM: Matching Ontologies with Large Language Models’. arXiv. Available at: https://doi.org/10.48550/arXiv.2404.10317.

* Patel, L. et al. (2024) ‘Semantic Operators: A Declarative Model for Rich, AI-based Analytics Over Text Data’. arXiv. Available at: https://doi.org/10.48550/arXiv.2407.11418.

## Products and Services

* NotebookLM

* Databricks Data+AI

* Allegro CL 11
